{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 Behavioural Cloning\n",
    "## Background\n",
    "\n",
    "\n",
    "## Rationale\n",
    "\n",
    "## Plan\n",
    "\n",
    "## Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recommended Flipping\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Cropping2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepImage(path):\n",
    "    source_path = path\n",
    "    filename = source_path.split('/')[-1]\n",
    "    current_path = './data/IMG/' + filename\n",
    "    return cv2.imread(current_path, 1) # 0 = grayscale, 1 = Colour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "lines = []\n",
    "with open('./data/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        lines.append(line)\n",
    "\n",
    "lines = lines[1:]\n",
    "        \n",
    "images = []\n",
    "measurements = []\n",
    "for line in lines:\n",
    "    # IMAGES\n",
    "    img_center = prepImage(line[0])\n",
    "    images.append(img_center)\n",
    "    \n",
    "    img_left = prepImage(line[1])\n",
    "    images.append(img_left)\n",
    "    \n",
    "    img_right = prepImage(line[2])\n",
    "    images.append(img_right)\n",
    "    \n",
    "    # STEERING\n",
    "    steering_center = float(line[3])\n",
    "    measurements.append(steering_center)\n",
    "    \n",
    "    correction = 0.2 # this is a parameter to tune\n",
    "    steering_left = steering_center + correction\n",
    "    measurements.append(steering_left)\n",
    "    steering_right = steering_center - correction\n",
    "    measurements.append(steering_right)\n",
    "    \n",
    "        \n",
    "# X_train = np.array(images)\n",
    "# y_train = np.array(measurements)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        \n",
    "# Flip the Images for more data   \n",
    "augmented_images, augmented_measurements = [] ,[]\n",
    "for image, measurement in zip(images, measurements ):\n",
    "    augmented_images.append(image)\n",
    "    augmented_measurements.append(measurement)\n",
    "    augmented_images.append(cv2.flip(image, 1))\n",
    "    augmented_measurements.append(measurement*-1.0)  \n",
    "        \n",
    "X_train = np.array(augmented_images)\n",
    "y_train = np.array(augmented_measurements)   \n",
    "\n",
    "del(images)\n",
    "del(measurements)\n",
    "del(augmented_images)\n",
    "del(augmented_measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.405977744"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(X_train) / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # ## Split into train and validation\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# # Create Data Generator\n",
    "# datagen = ImageDataGenerator(\n",
    "#     featurewise_center=True,\n",
    "#     featurewise_std_normalization=True) #,\n",
    "#     #rotation_range=20, # Could cause an issue for steering\n",
    "#     #width_shift_range=0.2, \n",
    "#     #height_shift_range=0.2 ) #,\n",
    "#     # horizontal_flip=True) # need to flip steering too\n",
    "\n",
    "# # compute quantities required for featurewise normalization\n",
    "# # (std, mean, and principal components if ZCA whitening is applied)\n",
    "# datagen.fit(X_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Recommended Flipping\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Cropping2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASIC MODEL (No Generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38572 samples, validate on 9644 samples\n",
      "Epoch 1/10\n",
      "38572/38572 [==============================] - 68s - loss: 0.0176 - val_loss: 0.0119\n",
      "Epoch 2/10\n",
      "38572/38572 [==============================] - 67s - loss: 0.0109 - val_loss: 0.0111\n",
      "Epoch 3/10\n",
      "38572/38572 [==============================] - 67s - loss: 0.0100 - val_loss: 0.0103\n",
      "Epoch 4/10\n",
      "38572/38572 [==============================] - 67s - loss: 0.0096 - val_loss: 0.0101\n",
      "Epoch 5/10\n",
      "38572/38572 [==============================] - 67s - loss: 0.0092 - val_loss: 0.0101\n",
      "Epoch 6/10\n",
      "38572/38572 [==============================] - 67s - loss: 0.0089 - val_loss: 0.0105\n",
      "Epoch 7/10\n",
      "38572/38572 [==============================] - 67s - loss: 0.0088 - val_loss: 0.0103\n",
      "Epoch 8/10\n",
      "38572/38572 [==============================] - 67s - loss: 0.0085 - val_loss: 0.0106\n",
      "Epoch 9/10\n",
      "38572/38572 [==============================] - 67s - loss: 0.0083 - val_loss: 0.0101\n",
      "Epoch 10/10\n",
      "38572/38572 [==============================] - 67s - loss: 0.0082 - val_loss: 0.0107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f52dfebbfd0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=5),\n",
    "    ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True, verbose=0)\n",
    "]\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 10\n",
    "learn_rate = 0.001\n",
    "drop_rate = 0.3\n",
    "\n",
    "model = Sequential()\n",
    "# Normalise\n",
    "model.add(Lambda(lambda x: (x / 255.0) - 0.5, input_shape=(160,320,3)))\n",
    "# Crop\n",
    "model.add(Cropping2D(cropping=((50,20), (0,0)), input_shape=(3,160,320)))\n",
    "\n",
    "# Conv Layer 1\n",
    "model.add(Conv2D(8, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(drop_rate))\n",
    "\n",
    "# Conv Layer 2\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(drop_rate))\n",
    "\n",
    "# Conv Layer 3\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(drop_rate))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(drop_rate))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=learn_rate))\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "history_object = model.fit(X_train, \n",
    "          y_train, \n",
    "          validation_split=0.2,\n",
    "          shuffle=True,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "# Save model in callbacks\n",
    "# model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do some plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### print the keys contained in the history object\n",
    "print(history_object.history.keys())\n",
    "\n",
    "### plot the training and validation loss for each epoch\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Level Model (Generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepImage(path, img_dir):\n",
    "    filename = path.split('/')[-1]\n",
    "    current_path = img_dir + filename\n",
    "    return cv2.imread(current_path, 1) # 0 = grayscale, 1 = Colour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# driving_log = './data/driving_log.csv'\n",
    "# img_dir = './data/IMG/'\n",
    "# driving_log = './car_training_data/driving_log.csv'\n",
    "# img_dir = './car_training_data/IMG/'\n",
    "driving_log = './data/driving_log_combined.csv'\n",
    "img_dir = './data/IMG/'\n",
    "\n",
    "samples = []\n",
    "with open(driving_log) as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    # Skip header row\n",
    "    next(reader)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Hyperparameters - leave these in this position\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "learn_rate = 0.001\n",
    "drop_rate = 0.3\n",
    "\n",
    "## Add multi camera angels (left/right) + Augmentation# Flip the Images for more data   \n",
    "\n",
    "def generator(samples, batch_size=32):\n",
    "    num_samples = len(samples)\n",
    "    batch_size = batch_size // 2 # to deal with image augmentation\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "            for batch_sample in batch_samples:\n",
    "                \n",
    "                # IMAGES\n",
    "                img_center = prepImage(batch_sample[0], img_dir)\n",
    "                images.append(img_center)\n",
    "                \n",
    "                img_left = prepImage(batch_sample[1], img_dir)\n",
    "                images.append(img_left)\n",
    "                \n",
    "                img_right = prepImage(batch_sample[2], img_dir)\n",
    "                images.append(img_right)\n",
    "                \n",
    "                \n",
    "                # STEERING\n",
    "                steering_center = float(batch_sample[3])\n",
    "                angles.append(steering_center)\n",
    "\n",
    "                correction = 1 # this is a parameter to tune 0.2\n",
    "                steering_left = steering_center + correction\n",
    "                angles.append(steering_left)\n",
    "                steering_right = steering_center - correction\n",
    "                angles.append(steering_right)\n",
    "        \n",
    "            # AUGMENTATION (Flip the Images for more data)\n",
    "            augmented_images = []\n",
    "            augmented_angles = [] \n",
    "            for image, angle in zip(images, angles):\n",
    "                augmented_images.append(image)\n",
    "                augmented_angles.append(angle)\n",
    "                augmented_images.append(cv2.flip(image, 1))\n",
    "                augmented_angles.append(angle*-1.0)  \n",
    "\n",
    "            X_train = np.array(augmented_images)\n",
    "            y_train = np.array(augmented_angles)\n",
    "            \n",
    "            yield shuffle(X_train, y_train)\n",
    "            \n",
    "# compile and train the model using the generator function\n",
    "train_generator = generator(train_samples, batch_size=batch_size)\n",
    "validation_generator = generator(validation_samples, batch_size=batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "521/520 [==============================] - 283s - loss: 0.1933 - val_loss: 0.0259\n",
      "Epoch 2/10\n",
      "521/520 [==============================] - 282s - loss: 0.0279 - val_loss: 0.0153\n",
      "Epoch 3/10\n",
      "521/520 [==============================] - 282s - loss: 0.0209 - val_loss: 0.0128\n",
      "Epoch 4/10\n",
      "521/520 [==============================] - 282s - loss: 0.0198 - val_loss: 0.0133\n",
      "Epoch 5/10\n",
      "521/520 [==============================] - 282s - loss: 0.0178 - val_loss: 0.0117\n",
      "Epoch 6/10\n",
      "521/520 [==============================] - 282s - loss: 0.0169 - val_loss: 0.0110\n",
      "Epoch 7/10\n",
      "521/520 [==============================] - 281s - loss: 0.0161 - val_loss: 0.0109\n",
      "Epoch 8/10\n",
      "521/520 [==============================] - 282s - loss: 0.0152 - val_loss: 0.0115\n",
      "Epoch 9/10\n",
      "521/520 [==============================] - 283s - loss: 0.0155 - val_loss: 0.0114\n",
      "Epoch 10/10\n",
      "521/520 [==============================] - 282s - loss: 0.0146 - val_loss: 0.0104\n"
     ]
    }
   ],
   "source": [
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=5),\n",
    "    ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True, verbose=0)\n",
    "]\n",
    "\n",
    "img_h, img_w, ch = 160, 320, 3\n",
    "top_crop, bottom_crop = 50, 20\n",
    "left_crop, right_crop = 0, 0\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# # Normalise\n",
    "model.add(Lambda(lambda x: (x / 255.0) - 0.5, input_shape=(img_h,img_w,ch)))\n",
    "# Crop\n",
    "model.add(Cropping2D(cropping=((top_crop, bottom_crop), (left_crop, right_crop)) )) # , input_shape=(3,160,320)))\n",
    "# Add Random Noise - Prevents overfitting\n",
    "model.add( GaussianNoise(stddev=1.0) )\n",
    "\n",
    "# Conv Layer 1\n",
    "model.add(Conv2D(24, (5, 5), strides=(2,2), activation='relu', padding='same')) # filters = 8\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(drop_rate))\n",
    "\n",
    "# Conv Layer 2\n",
    "model.add(Conv2D(36, (5, 5), strides=(2,2), activation='relu', padding='same')) # filters = 16\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(drop_rate))\n",
    "\n",
    "# Conv Layer 3\n",
    "model.add(Conv2D(48, (3, 3), strides=(1,1), activation='relu', padding='same')) # filters = 32\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(drop_rate))\n",
    "\n",
    "# Conv Layer 4\n",
    "model.add(Conv2D(64, (3, 3), strides=(1,1), activation='relu', padding='same')) # filters = 64\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(drop_rate))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "#model.add(Dropout(drop_rate))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=learn_rate))\n",
    "\n",
    "# num train samples * 3 for centre, left, right camera, *2 for augmentation\n",
    "batch_step_factor = 3*2 # Need 3*2 to use full dataset each epoch\n",
    "\n",
    "history_object = model.fit_generator(train_generator,\n",
    "                    # num train samples * 3 for centre, left, right camera, *2 for augmentation\n",
    "                    steps_per_epoch= (len(train_samples)*batch_step_factor) / batch_size, \n",
    "                    validation_data=validation_generator, \n",
    "                    validation_steps= (len(validation_samples)*batch_step_factor) / batch_size, \n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEZCAYAAABiu9n+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlJQkREQkJSUJERFJ6f8DEfrF4Aqk5jUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f81655c5278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### print the keys contained in the history object\n",
    "print(history_object.history.keys())\n",
    "\n",
    "### plot the training and validation loss for each epoch\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Working code with left, right and centre images\n",
    "##\n",
    "\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import sklearn\n",
    "# from sklearn.utils import shuffle\n",
    "\n",
    "# # Hyperparameters - leave these in this position\n",
    "# batch_size = 256\n",
    "# epochs = 5\n",
    "# learn_rate = 0.001\n",
    "# drop_rate = 0.3\n",
    "\n",
    "# ## Add multi camera angels (left/right) + Augmentation# Flip the Images for more data   \n",
    "\n",
    "# def generator(samples, batch_size=32):\n",
    "#     num_samples = len(samples)\n",
    "#     while 1: # Loop forever so the generator never terminates\n",
    "#         shuffle(samples)\n",
    "#         for offset in range(0, num_samples, batch_size):\n",
    "#             batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "#             images = []\n",
    "#             angles = []\n",
    "#             for batch_sample in batch_samples:\n",
    "                \n",
    "#                 # IMAGES\n",
    "#                 img_center = prepImage(batch_sample[0], img_dir)\n",
    "#                 images.append(img_center)\n",
    "                \n",
    "#                 img_left = prepImage(batch_sample[1], img_dir)\n",
    "#                 images.append(img_left)\n",
    "                \n",
    "#                 img_right = prepImage(batch_sample[2], img_dir)\n",
    "#                 images.append(img_right)\n",
    "                \n",
    "                \n",
    "#                 # STEERING\n",
    "#                 steering_center = float(batch_sample[3])\n",
    "#                 angles.append(steering_center)\n",
    "\n",
    "#                 correction = 0.2 # this is a parameter to tune\n",
    "#                 steering_left = steering_center + correction\n",
    "#                 angles.append(steering_left)\n",
    "#                 steering_right = steering_center - correction\n",
    "#                 angles.append(steering_right)\n",
    "\n",
    "\n",
    "#             # trim image to only see section with road\n",
    "#             X_train = np.array(images)\n",
    "#             y_train = np.array(angles)\n",
    "            \n",
    "#             yield shuffle(X_train, y_train)\n",
    "            \n",
    "# # compile and train the model using the generator function\n",
    "# train_generator = generator(train_samples, batch_size=batch_size)\n",
    "# validation_generator = generator(validation_samples, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Working code with centre images\n",
    "##\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import sklearn\n",
    "# from sklearn.utils import shuffle\n",
    "\n",
    "# # Hyperparameters - leave these in this position\n",
    "# batch_size = 256\n",
    "# epochs = 10\n",
    "# learn_rate = 0.001\n",
    "# drop_rate = 0.3\n",
    "\n",
    "# ## Add multi camera angels (left/right) + Augmentation# Flip the Images for more data   \n",
    "\n",
    "# def generator(samples, batch_size=32):\n",
    "#     num_samples = len(samples)\n",
    "#     while 1: # Loop forever so the generator never terminates\n",
    "#         shuffle(samples)\n",
    "#         for offset in range(0, num_samples, batch_size):\n",
    "#             batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "#             images = []\n",
    "#             angles = []\n",
    "#             for batch_sample in batch_samples:\n",
    "#                 name = img_dir + batch_sample[0].split('/')[-1]\n",
    "#                 # Update this for all camera angles\n",
    "#                 center_image = cv2.imread(name, 1) # 0 = grayscale, 1 = Colour\n",
    "#                 center_angle = float(batch_sample[3])\n",
    "#                 images.append(center_image)\n",
    "#                 angles.append(center_angle)\n",
    "\n",
    "#             # trim image to only see section with road\n",
    "#             X_train = np.array(images)\n",
    "#             y_train = np.array(angles)\n",
    "#             yield shuffle(X_train, y_train)\n",
    "            \n",
    "# # compile and train the model using the generator function\n",
    "# train_generator = generator(train_samples, batch_size=batch_size)\n",
    "# validation_generator = generator(validation_samples, batch_size=batch_size)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
