{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 Behavioural Cloning\n",
    "## Background\n",
    "\n",
    "\n",
    "## Rationale\n",
    "\n",
    "## Plan\n",
    "\n",
    "## Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Recommended Flipping\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Cropping2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepImage(path):\n",
    "    source_path = path\n",
    "    filename = source_path.split('/')[-1]\n",
    "    current_path = './data/IMG/' + filename\n",
    "    return cv2.imread(current_path, 1) # 0 = grayscale, 1 = Colour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "lines = []\n",
    "with open('./data/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        lines.append(line)\n",
    "\n",
    "lines = lines[1:]\n",
    "        \n",
    "images = []\n",
    "measurements = []\n",
    "for line in lines:\n",
    "    # IMAGES\n",
    "    img_center = prepImage(line[0])\n",
    "    images.append(img_center)\n",
    "    \n",
    "    img_left = prepImage(line[1])\n",
    "    images.append(img_left)\n",
    "    \n",
    "    img_right = prepImage(line[2])\n",
    "    images.append(img_right)\n",
    "    \n",
    "    # STEERING\n",
    "    steering_center = float(line[3])\n",
    "    measurements.append(steering_center)\n",
    "    \n",
    "    correction = 0.2 # this is a parameter to tune\n",
    "    steering_left = steering_center + correction\n",
    "    measurements.append(steering_left)\n",
    "    steering_right = steering_center - correction\n",
    "    measurements.append(steering_right)\n",
    "    \n",
    "        \n",
    "# X_train = np.array(images)\n",
    "# y_train = np.array(measurements)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        \n",
    "# Flip the Images for more data   \n",
    "augmented_images, augmented_measurements = [] ,[]\n",
    "for image, measurement in zip(images, measurements ):\n",
    "    augmented_images.append(image)\n",
    "    augmented_measurements.append(measurement)\n",
    "    augmented_images.append(cv2.flip(image, 1))\n",
    "    augmented_measurements.append(measurement*-1.0)  \n",
    "        \n",
    "X_train = np.array(augmented_images)\n",
    "y_train = np.array(augmented_measurements)   \n",
    "\n",
    "del(images)\n",
    "del(measurements)\n",
    "del(augmented_images)\n",
    "del(augmented_measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.405977744"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(X_train) / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # ## Split into train and validation\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# # Create Data Generator\n",
    "# datagen = ImageDataGenerator(\n",
    "#     featurewise_center=True,\n",
    "#     featurewise_std_normalization=True) #,\n",
    "#     #rotation_range=20, # Could cause an issue for steering\n",
    "#     #width_shift_range=0.2, \n",
    "#     #height_shift_range=0.2 ) #,\n",
    "#     # horizontal_flip=True) # need to flip steering too\n",
    "\n",
    "# # compute quantities required for featurewise normalization\n",
    "# # (std, mean, and principal components if ZCA whitening is applied)\n",
    "# datagen.fit(X_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Recommended Flipping\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Cropping2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASIC MODEL (No Generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38572 samples, validate on 9644 samples\n",
      "Epoch 1/10\n",
      "38572/38572 [==============================] - 68s - loss: 0.0176 - val_loss: 0.0119\n",
      "Epoch 2/10\n",
      "38572/38572 [==============================] - 67s - loss: 0.0109 - val_loss: 0.0111\n",
      "Epoch 3/10\n",
      "38572/38572 [==============================] - 67s - loss: 0.0100 - val_loss: 0.0103\n",
      "Epoch 4/10\n",
      "38572/38572 [==============================] - 67s - loss: 0.0096 - val_loss: 0.0101\n",
      "Epoch 5/10\n",
      "38572/38572 [==============================] - 67s - loss: 0.0092 - val_loss: 0.0101\n",
      "Epoch 6/10\n",
      "38572/38572 [==============================] - 67s - loss: 0.0089 - val_loss: 0.0105\n",
      "Epoch 7/10\n",
      "38572/38572 [==============================] - 67s - loss: 0.0088 - val_loss: 0.0103\n",
      "Epoch 8/10\n",
      "38572/38572 [==============================] - 67s - loss: 0.0085 - val_loss: 0.0106\n",
      "Epoch 9/10\n",
      "38572/38572 [==============================] - 67s - loss: 0.0083 - val_loss: 0.0101\n",
      "Epoch 10/10\n",
      "38572/38572 [==============================] - 67s - loss: 0.0082 - val_loss: 0.0107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f52dfebbfd0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=5),\n",
    "    ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True, verbose=0)\n",
    "]\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 10\n",
    "learn_rate = 0.001\n",
    "drop_rate = 0.3\n",
    "\n",
    "model = Sequential()\n",
    "# Normalise\n",
    "model.add(Lambda(lambda x: (x / 255.0) - 0.5, input_shape=(160,320,3)))\n",
    "# Crop\n",
    "model.add(Cropping2D(cropping=((50,20), (0,0)), input_shape=(3,160,320)))\n",
    "\n",
    "# Conv Layer 1\n",
    "model.add(Conv2D(8, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(drop_rate))\n",
    "\n",
    "# Conv Layer 2\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(drop_rate))\n",
    "\n",
    "# Conv Layer 3\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(drop_rate))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(drop_rate))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=learn_rate))\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "history_object = model.fit(X_train, \n",
    "          y_train, \n",
    "          validation_split=0.2,\n",
    "          shuffle=True,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "# Save model in callbacks\n",
    "# model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do some plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### print the keys contained in the history object\n",
    "print(history_object.history.keys())\n",
    "\n",
    "### plot the training and validation loss for each epoch\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Level Model (Generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepImage(path, img_dir):\n",
    "    filename = path.split('/')[-1]\n",
    "    current_path = img_dir + filename\n",
    "    return cv2.imread(current_path, 1) # 0 = grayscale, 1 = Colour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# driving_log = './data/driving_log.csv'\n",
    "# img_dir = './data/IMG/'\n",
    "driving_log = './car_training_data/driving_log.csv'\n",
    "img_dir = './car_training_data/IMG/'\n",
    "\n",
    "samples = []\n",
    "with open(driving_log) as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Hyperparameters - leave these in this position\n",
    "batch_size = 256\n",
    "epochs = 15\n",
    "learn_rate = 0.001\n",
    "drop_rate = 0.3\n",
    "\n",
    "## Add multi camera angels (left/right) + Augmentation# Flip the Images for more data   \n",
    "\n",
    "def generator(samples, batch_size=32):\n",
    "    num_samples = len(samples)\n",
    "    batch_size = batch_size // 2 # to deal with image augmentation\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "            for batch_sample in batch_samples:\n",
    "                \n",
    "                # IMAGES\n",
    "                img_center = prepImage(batch_sample[0], img_dir)\n",
    "                images.append(img_center)\n",
    "                \n",
    "                img_left = prepImage(batch_sample[1], img_dir)\n",
    "                images.append(img_left)\n",
    "                \n",
    "                img_right = prepImage(batch_sample[2], img_dir)\n",
    "                images.append(img_right)\n",
    "                \n",
    "                \n",
    "                # STEERING\n",
    "                steering_center = float(batch_sample[3])\n",
    "                angles.append(steering_center)\n",
    "\n",
    "                correction = 0.2 # this is a parameter to tune\n",
    "                steering_left = steering_center + correction\n",
    "                angles.append(steering_left)\n",
    "                steering_right = steering_center - correction\n",
    "                angles.append(steering_right)\n",
    "        \n",
    "            # AUGMENTATION (Flip the Images for more data)\n",
    "            augmented_images = []\n",
    "            augmented_angles = [] \n",
    "            for image, angle in zip(images, angles):\n",
    "                augmented_images.append(image)\n",
    "                augmented_angles.append(angle)\n",
    "                augmented_images.append(cv2.flip(image, 1))\n",
    "                augmented_angles.append(angle*-1.0)  \n",
    "\n",
    "            X_train = np.array(augmented_images)\n",
    "            y_train = np.array(augmented_angles)\n",
    "            \n",
    "            yield shuffle(X_train, y_train)\n",
    "            \n",
    "# compile and train the model using the generator function\n",
    "train_generator = generator(train_samples, batch_size=batch_size)\n",
    "validation_generator = generator(validation_samples, batch_size=batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Working code with left, right and centre images\n",
    "##\n",
    "\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import sklearn\n",
    "# from sklearn.utils import shuffle\n",
    "\n",
    "# # Hyperparameters - leave these in this position\n",
    "# batch_size = 256\n",
    "# epochs = 5\n",
    "# learn_rate = 0.001\n",
    "# drop_rate = 0.3\n",
    "\n",
    "# ## Add multi camera angels (left/right) + Augmentation# Flip the Images for more data   \n",
    "\n",
    "# def generator(samples, batch_size=32):\n",
    "#     num_samples = len(samples)\n",
    "#     while 1: # Loop forever so the generator never terminates\n",
    "#         shuffle(samples)\n",
    "#         for offset in range(0, num_samples, batch_size):\n",
    "#             batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "#             images = []\n",
    "#             angles = []\n",
    "#             for batch_sample in batch_samples:\n",
    "                \n",
    "#                 # IMAGES\n",
    "#                 img_center = prepImage(batch_sample[0], img_dir)\n",
    "#                 images.append(img_center)\n",
    "                \n",
    "#                 img_left = prepImage(batch_sample[1], img_dir)\n",
    "#                 images.append(img_left)\n",
    "                \n",
    "#                 img_right = prepImage(batch_sample[2], img_dir)\n",
    "#                 images.append(img_right)\n",
    "                \n",
    "                \n",
    "#                 # STEERING\n",
    "#                 steering_center = float(batch_sample[3])\n",
    "#                 angles.append(steering_center)\n",
    "\n",
    "#                 correction = 0.2 # this is a parameter to tune\n",
    "#                 steering_left = steering_center + correction\n",
    "#                 angles.append(steering_left)\n",
    "#                 steering_right = steering_center - correction\n",
    "#                 angles.append(steering_right)\n",
    "\n",
    "\n",
    "#             # trim image to only see section with road\n",
    "#             X_train = np.array(images)\n",
    "#             y_train = np.array(angles)\n",
    "            \n",
    "#             yield shuffle(X_train, y_train)\n",
    "            \n",
    "# # compile and train the model using the generator function\n",
    "# train_generator = generator(train_samples, batch_size=batch_size)\n",
    "# validation_generator = generator(validation_samples, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Working code with centre images\n",
    "##\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import sklearn\n",
    "# from sklearn.utils import shuffle\n",
    "\n",
    "# # Hyperparameters - leave these in this position\n",
    "# batch_size = 256\n",
    "# epochs = 10\n",
    "# learn_rate = 0.001\n",
    "# drop_rate = 0.3\n",
    "\n",
    "# ## Add multi camera angels (left/right) + Augmentation# Flip the Images for more data   \n",
    "\n",
    "# def generator(samples, batch_size=32):\n",
    "#     num_samples = len(samples)\n",
    "#     while 1: # Loop forever so the generator never terminates\n",
    "#         shuffle(samples)\n",
    "#         for offset in range(0, num_samples, batch_size):\n",
    "#             batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "#             images = []\n",
    "#             angles = []\n",
    "#             for batch_sample in batch_samples:\n",
    "#                 name = img_dir + batch_sample[0].split('/')[-1]\n",
    "#                 # Update this for all camera angles\n",
    "#                 center_image = cv2.imread(name, 1) # 0 = grayscale, 1 = Colour\n",
    "#                 center_angle = float(batch_sample[3])\n",
    "#                 images.append(center_image)\n",
    "#                 angles.append(center_angle)\n",
    "\n",
    "#             # trim image to only see section with road\n",
    "#             X_train = np.array(images)\n",
    "#             y_train = np.array(angles)\n",
    "#             yield shuffle(X_train, y_train)\n",
    "            \n",
    "# # compile and train the model using the generator function\n",
    "# train_generator = generator(train_samples, batch_size=batch_size)\n",
    "# validation_generator = generator(validation_samples, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      " 26/109 [======>.......................] - ETA: 113s - loss: 0.0713"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-4e79f80d266c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                     callbacks=callbacks)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1108\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1888\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1889\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1631\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1633\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1634\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2229\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=5),\n",
    "    ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True, verbose=0)\n",
    "]\n",
    "\n",
    "img_h, img_w, ch = 160, 320, 3\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# # Normalise\n",
    "model.add(Lambda(lambda x: (x / 255.0) - 0.5, input_shape=(img_h,img_w,ch)))\n",
    "# Crop\n",
    "model.add(Cropping2D(cropping=((50,20), (0,0)) )) # , input_shape=(3,160,320)))\n",
    "\n",
    "# Conv Layer 1\n",
    "model.add(Conv2D(8, (3, 3), activation='relu', padding='same')) # filters = 8\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(drop_rate))\n",
    "\n",
    "# Conv Layer 2\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', padding='same')) # filters = 16\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(drop_rate))\n",
    "\n",
    "# Conv Layer 3\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same')) # filters = 32\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(drop_rate))\n",
    "\n",
    "# Conv Layer 4\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same')) # filters = 64\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(drop_rate))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dropout(drop_rate))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=learn_rate))\n",
    "\n",
    "history_object = model.fit_generator(train_generator,\n",
    "                    # num train samples * 3 for centre, left, right camera, *2 for augmentation\n",
    "                    steps_per_epoch= (len(train_samples)*3*2) / batch_size, \n",
    "                    validation_data=validation_generator, \n",
    "                    validation_steps= (len(validation_samples)*3*2) / batch_size, \n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'val_loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEZCAYAAACAZ8KHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd8508c5208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### print the keys contained in the history object\n",
    "print(history_object.history.keys())\n",
    "\n",
    "### plot the training and validation loss for each epoch\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.27734375"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_samples)/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
